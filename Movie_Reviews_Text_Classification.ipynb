{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data,train_labels),(test_data,test_labels)=data.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = data.get_word_index()\n",
    "word_index={k:(v+3) for k,v,in word_index.items()}\n",
    "#for unvalid words\n",
    "word_index[\"<PAD>\"]=0 #Padding to make each movie rivew equal\n",
    "word_index[\"<START>\"]=1\n",
    "word_index[\"<UNK>\"]=2 #unknown\n",
    "word_index[\"<UNUSED>\"]=3\n",
    "\n",
    "reverse_word_index=dict([(value,key) for (key,value) in word_index.items()])\n",
    "#swap all the keys and value so integers point to the word, not the other wat round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i,\"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index[\"<PAD>\"], padding='post',maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index[\"<PAD>\"], padding='post',maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  249, 1323,    7,   61,  113,   10,   10,   13, 1637,   14,\n",
       "         20,   56,   33, 2401,   18,  457,   88,   13, 2626, 1400,   45,\n",
       "       3171,   13,   70,   79,   49,  706,  919,   13,   16,  355,  340,\n",
       "        355, 1696,   96,  143,    4,   22,   32,  289,    7,   61,  369,\n",
       "         71, 2359,    5,   13,   16,  131, 2073,  249,  114,  249,  229,\n",
       "        249,   20,   13,   28,  126,  110,   13,  473,    8,  569,   61,\n",
       "        419,   56,  429,    6, 1513,   18,   35,  534,   95,  474,  570,\n",
       "          5,   25,  124,  138,   88,   12,  421, 1543,   52,  725, 6397,\n",
       "         61,  419,   11,   13, 1571,   15, 1543,   20,   11,    4,    2,\n",
       "          5,  296,   12, 3524,    5,   15,  421,  128,   74,  233,  334,\n",
       "        207,  126,  224,   12,  562,  298, 2167, 1272,    7, 2601,    5,\n",
       "        516,  988,   43,    8,   79,  120,   15,  595,   13,  784,   25,\n",
       "       3171,   18,  165,  170,  143,   19,   14,    5, 7224,    6,  226,\n",
       "        251,    7,   61,  113,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(train_data[4])\n",
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 250)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]),len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(10000,16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "15000/15000 [==============================] - 3s 167us/sample - loss: 0.6918 - accuracy: 0.6055 - val_loss: 0.6896 - val_accuracy: 0.7072\n",
      "Epoch 2/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.6859 - accuracy: 0.6776 - val_loss: 0.6812 - val_accuracy: 0.6897\n",
      "Epoch 3/40\n",
      "15000/15000 [==============================] - 1s 42us/sample - loss: 0.6732 - accuracy: 0.7250 - val_loss: 0.6651 - val_accuracy: 0.6904\n",
      "Epoch 4/40\n",
      "15000/15000 [==============================] - 1s 40us/sample - loss: 0.6515 - accuracy: 0.7499 - val_loss: 0.6402 - val_accuracy: 0.7475\n",
      "Epoch 5/40\n",
      "15000/15000 [==============================] - 1s 43us/sample - loss: 0.6193 - accuracy: 0.7873 - val_loss: 0.6063 - val_accuracy: 0.7734\n",
      "Epoch 6/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.5783 - accuracy: 0.8103 - val_loss: 0.5656 - val_accuracy: 0.8058\n",
      "Epoch 7/40\n",
      "15000/15000 [==============================] - 1s 41us/sample - loss: 0.5319 - accuracy: 0.8329 - val_loss: 0.5230 - val_accuracy: 0.8177\n",
      "Epoch 8/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.4850 - accuracy: 0.8488 - val_loss: 0.4820 - val_accuracy: 0.8357\n",
      "Epoch 9/40\n",
      "15000/15000 [==============================] - 1s 36us/sample - loss: 0.4411 - accuracy: 0.8631 - val_loss: 0.4461 - val_accuracy: 0.8447\n",
      "Epoch 10/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.4020 - accuracy: 0.8759 - val_loss: 0.4143 - val_accuracy: 0.8525\n",
      "Epoch 11/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.3689 - accuracy: 0.8828 - val_loss: 0.3889 - val_accuracy: 0.8601\n",
      "Epoch 12/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.3408 - accuracy: 0.8901 - val_loss: 0.3680 - val_accuracy: 0.8647\n",
      "Epoch 13/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.3168 - accuracy: 0.8957 - val_loss: 0.3514 - val_accuracy: 0.8683\n",
      "Epoch 14/40\n",
      "15000/15000 [==============================] - 1s 36us/sample - loss: 0.2966 - accuracy: 0.9027 - val_loss: 0.3388 - val_accuracy: 0.8713\n",
      "Epoch 15/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.2785 - accuracy: 0.9061 - val_loss: 0.3281 - val_accuracy: 0.8725\n",
      "Epoch 16/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.2629 - accuracy: 0.9109 - val_loss: 0.3179 - val_accuracy: 0.8761\n",
      "Epoch 17/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.2491 - accuracy: 0.9151 - val_loss: 0.3108 - val_accuracy: 0.8785\n",
      "Epoch 18/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.2366 - accuracy: 0.9195 - val_loss: 0.3042 - val_accuracy: 0.8800\n",
      "Epoch 19/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.2248 - accuracy: 0.9241 - val_loss: 0.2994 - val_accuracy: 0.8815\n",
      "Epoch 20/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.2138 - accuracy: 0.9273 - val_loss: 0.2951 - val_accuracy: 0.8829\n",
      "Epoch 21/40\n",
      "15000/15000 [==============================] - 1s 36us/sample - loss: 0.2043 - accuracy: 0.9321 - val_loss: 0.2921 - val_accuracy: 0.8827\n",
      "Epoch 22/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1950 - accuracy: 0.9354 - val_loss: 0.2892 - val_accuracy: 0.8835\n",
      "Epoch 23/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.1866 - accuracy: 0.9399 - val_loss: 0.2881 - val_accuracy: 0.8837\n",
      "Epoch 24/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1795 - accuracy: 0.9411 - val_loss: 0.2856 - val_accuracy: 0.8858\n",
      "Epoch 25/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.1716 - accuracy: 0.9451 - val_loss: 0.2846 - val_accuracy: 0.8857\n",
      "Epoch 26/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.1644 - accuracy: 0.9484 - val_loss: 0.2843 - val_accuracy: 0.8851\n",
      "Epoch 27/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1577 - accuracy: 0.9512 - val_loss: 0.2837 - val_accuracy: 0.8864\n",
      "Epoch 28/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.1514 - accuracy: 0.9537 - val_loss: 0.2841 - val_accuracy: 0.8874\n",
      "Epoch 29/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1458 - accuracy: 0.9559 - val_loss: 0.2846 - val_accuracy: 0.8872\n",
      "Epoch 30/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1406 - accuracy: 0.9577 - val_loss: 0.2855 - val_accuracy: 0.8875\n",
      "Epoch 31/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.1349 - accuracy: 0.9603 - val_loss: 0.2873 - val_accuracy: 0.8859\n",
      "Epoch 32/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.1300 - accuracy: 0.9629 - val_loss: 0.2890 - val_accuracy: 0.8864\n",
      "Epoch 33/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.1253 - accuracy: 0.9642 - val_loss: 0.2894 - val_accuracy: 0.8874\n",
      "Epoch 34/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1208 - accuracy: 0.9659 - val_loss: 0.2922 - val_accuracy: 0.8867\n",
      "Epoch 35/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.1161 - accuracy: 0.9680 - val_loss: 0.2931 - val_accuracy: 0.8872\n",
      "Epoch 36/40\n",
      "15000/15000 [==============================] - 1s 39us/sample - loss: 0.1118 - accuracy: 0.9689 - val_loss: 0.2957 - val_accuracy: 0.8872\n",
      "Epoch 37/40\n",
      "15000/15000 [==============================] - 1s 42us/sample - loss: 0.1078 - accuracy: 0.9717 - val_loss: 0.2986 - val_accuracy: 0.8842\n",
      "Epoch 38/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.1041 - accuracy: 0.9719 - val_loss: 0.3006 - val_accuracy: 0.8851\n",
      "Epoch 39/40\n",
      "15000/15000 [==============================] - 1s 37us/sample - loss: 0.1002 - accuracy: 0.9733 - val_loss: 0.3030 - val_accuracy: 0.8844\n",
      "Epoch 40/40\n",
      "15000/15000 [==============================] - 1s 38us/sample - loss: 0.0967 - accuracy: 0.9753 - val_loss: 0.3062 - val_accuracy: 0.8839\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "x_val=train_data[:10000]\n",
    "x_train=train_data[10000:]\n",
    "\n",
    "y_val=train_labels[:10000]\n",
    "y_train=train_labels[10000:]\n",
    "\n",
    "fitModel= model.fit(x_train,y_train,epochs=40,batch_size=512,validation_data=(x_val,y_val),verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 65us/sample - loss: 0.3273 - accuracy: 0.8725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3273283127260208, 0.87252]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=model.evaluate(test_data,test_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "<START> i thought this would be a fun comedy with no sex or nudity in it well there's no sexual content but as for being a fun comedy i only remember laughing a total three times during the green <UNK> the hilarity is centered around <UNK> and mental <UNK> which is just sick humor in a bad way br br i should have walked out of this film and i'm kicking myself because i didn't in fact i shouldn't have seen this movie in the first place the only good that came out of this film was was the fact that and his brother finally made <UNK> and put their ugly past behind them i advise any well meaning individual to steer clear of this gross out flick <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction: [6.8864194e-09]\n",
      "Actual: 0\n"
     ]
    }
   ],
   "source": [
    "test_review= test_data[150]\n",
    "predict=model.predict([test_review])\n",
    "print('Review: ')\n",
    "print(decode_review(test_review))\n",
    "print('Prediction: '+ str(predict[0]))\n",
    "print('Actual: '+ str(test_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shriyaroot/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: TextClassificationModel.hf/assets\n"
     ]
    }
   ],
   "source": [
    "#saving the model\n",
    "model.save('TextClassificationModel.hf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(s):\n",
    "    encoded=[1]\n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])         \n",
    "        else:\n",
    "            encoded.append(2) \n",
    "    print (encoded)        \n",
    "    return encoded   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.saving.saved_model.load.Sequential at 0x7fc3aabf0e90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1=keras.models.load_model(\"TextClassificationModel.hf\")\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 830, 2023, 13, 590, 2, 13, 590, 2, 1604, 3363, 963, 2, 1604, 1209, 2, 830, 2023, 963, 2, 503, 963, 590, 830, 2, 1209, 13, 2014, 1983, 590, 2, 963, 1964, 963, 1479, 2, 1983, 6, 1095, 963, 2, 2, 13, 2, 1992, 6, 830, 1148, 2023, 963, 1095, 2, 13, 830, 2, 6, 2, 1209, 963, 1992, 2, 1095, 6, 5135, 590, 2, 6, 1331, 1604, 2, 6, 3363, 1095, 2, 13, 2, 590, 830, 13, 2014, 2014, 2, 2023, 6, 1964, 963, 2, 830, 2023, 963, 2, 1983, 1604, 1964, 13, 963, 2, 1479, 1206, 3363, 3363, 13, 3363, 1331, 2, 830, 2023, 1479, 1604, 1206, 1331, 2023, 2, 1983, 5135, 2, 2023, 963, 6, 1095, 2, 13, 830, 2, 13, 590, 2, 590, 1604, 2, 6, 1983, 6, 4543, 13, 3363, 1331, 2, 830, 2023, 6, 830, 2, 13, 830, 2, 2023, 6, 590, 2, 6, 2, 2014, 6, 590, 830, 13, 3363, 1331, 2, 13, 1983, 1657, 1479, 963, 590, 590, 13, 1604, 3363, 1604, 3363, 1148, 963, 2, 6, 1331, 6, 13, 3363, 2, 590, 2023, 6, 2023, 2, 1479, 1206, 2295, 2023, 2, 2295, 2023, 6, 3363, 2, 590, 830, 963, 6, 2014, 590, 2, 830, 2023, 963, 2, 590, 1657, 1604, 830, 2014, 13, 1331, 2023, 830, 2, 6, 3363, 1095, 2, 1095, 1604, 963, 590, 2, 6, 2, 1983, 6, 1331, 3363, 13, 1209, 13, 1148, 963, 3363, 830, 2, 1657, 963, 1479, 1209, 1604, 1479, 1983, 6, 3363, 1148, 963, 2, 6, 590, 2, 830, 2023, 963, 2, 503, 963, 6, 1206, 830, 13, 1209, 1206, 2014, 2, 6, 3363, 1095, 2, 590, 1992, 963, 963, 830, 2, 6, 1983, 6, 3363, 2, 1983, 6, 830, 2023, 1206, 1479, 503, 5135, 2, 830, 2023, 963, 2, 1992, 6, 5135, 2, 13, 2, 1992, 6, 590, 2, 830, 2023, 13, 3363, 2295, 13, 3363, 1331, 2, 2, 1992, 2023, 6, 830, 2, 6, 2, 1209, 1206, 3363, 3363, 5135, 2, 1983, 1604, 1964, 13, 963, 2, 2, 1206, 3363, 830, 13, 2014, 2, 830, 2023, 963, 2, 963, 3363, 1095, 13, 830, 2, 1468, 1206, 590, 830, 2, 1331, 963, 830, 590, 2, 590, 6, 1095, 2, 6, 3363, 1095, 2, 1983, 963, 2, 503, 963, 13, 3363, 1331, 2, 6, 2, 1479, 963, 6, 2014, 2014, 5135, 2, 590, 963, 3363, 590, 13, 830, 13, 1964, 963, 2, 1657, 963, 1479, 590, 1604, 3363, 2, 1992, 1604, 1992, 13, 830, 2, 1468, 1206, 590, 830, 2, 503, 2014, 1604, 1992, 590, 2, 5135, 1604, 1206, 2, 6, 1992, 6, 5135, 2, 590, 2023, 6, 2023, 2, 1479, 1206, 2295, 2023, 2, 2295, 2023, 6, 3363, 758, 590, 2, 503, 1479, 13, 2014, 2014, 13, 6, 3363, 830, 2, 1657, 963, 1479, 1209, 1604, 1479, 1983, 6, 3363, 1148, 963, 2, 1992, 13, 2014, 2014, 2, 590, 830, 6, 5135, 2, 1992, 13, 830, 2023, 2, 5135, 1604, 1206, 2, 1209, 1604, 1479, 963, 1964, 963, 1479, 13, 2, 1992, 6, 590, 2, 1206, 1657, 2, 6, 830, 2, 241, 342, 2241, 2, 13, 3363, 2, 830, 2023, 963, 2, 1983, 1604, 1479, 3363, 13, 3363, 1331, 2, 830, 13, 590, 590, 1206, 963, 590, 2, 590, 830, 1479, 963, 1992, 3363, 2, 6, 2014, 2014, 2, 6, 1479, 1604, 1206, 3363, 1095, 2, 1983, 963, 2, 1992, 13, 830, 2023, 2, 830, 963, 6, 1479, 590, 2, 1468, 1206, 590, 830, 2, 1657, 1604, 1206, 1479, 13, 3363, 1331, 2, 1095, 1604, 1992, 3363, 2, 1983, 5135, 2, 1209, 6, 1148, 963, 2, 830, 2023, 963, 2, 1983, 1604, 1983, 963, 3363, 830, 2, 5135, 1604, 1206, 2, 830, 1479, 5135, 2, 830, 1604, 2, 590, 830, 1604, 1657, 2, 1148, 1479, 5135, 13, 3363, 1331, 2, 6, 3363, 1604, 830, 2023, 963, 1479, 2, 590, 1148, 963, 3363, 963, 2, 1148, 1604, 1983, 963, 590, 2, 1604, 3363, 2, 6, 3363, 1095, 2, 5135, 1604, 1206, 2, 590, 830, 6, 1479, 830, 2, 6, 1331, 6, 13, 3363, 2, 590, 963, 1479, 13, 1604, 1206, 590, 2014, 5135, 2, 13, 758, 1983, 2, 1479, 963, 6, 2014, 2014, 5135, 2, 590, 963, 3363, 590, 13, 830, 13, 1964, 963, 2, 590, 1604, 2, 3363, 1604, 2, 590, 1206, 1479, 1657, 1479, 13, 590, 963, 2, 503, 1206, 830, 2, 13, 2, 1479, 963, 1148, 2295, 1604, 3363, 2, 830, 2023, 13, 590, 2, 1209, 13, 2014, 1983, 2, 1992, 6, 590, 2, 830, 2023, 963, 2, 1209, 13, 1479, 590, 830, 2, 1209, 13, 2014, 1983, 2, 830, 2023, 6, 830, 2, 6, 1148, 830, 1206, 6, 2014, 2014, 5135, 2, 1983, 6, 1095, 963, 2, 1983, 963, 2, 1148, 1479, 5135, 2, 830, 2023, 6, 830, 2, 1983, 1206, 1148, 2023, 2, 13, 2, 1468, 1206, 590, 830, 2, 1148, 1604, 1206, 2014, 1095, 3363, 758, 830, 2, 590, 830, 1604, 1657, 2, 2, 2, 830, 2023, 13, 590, 2, 13, 590, 2, 6, 2, 1983, 1206, 590, 830, 2, 1209, 1604, 1479, 2, 963, 1964, 963, 1479, 5135, 1604, 3363, 963, 13, 3363, 1148, 2014, 1206, 1095, 13, 3363, 1331, 2, 5135, 1604, 1206, 2, 590, 963, 3363, 590, 13, 830, 13, 1964, 963, 2, 1657, 963, 1604, 1657, 2014, 963, 2, 1604, 1206, 830, 2, 830, 2023, 963, 1479, 963, 2, 2, 963, 1964, 963, 1479, 5135, 1604, 3363, 963, 2, 1983, 1206, 590, 830, 2, 1992, 6, 830, 1148, 2023, 2, 830, 2023, 13, 590, 2, 1209, 13, 2014, 1983, 2, 2, 13, 830, 758, 590, 2, 1468, 1206, 590, 830, 2, 503, 963, 6, 1206, 830, 13, 1209, 1206, 2014, 2, 6, 3363, 1095, 2, 13, 830, 2, 1992, 6, 590, 2, 6, 1148, 830, 1206, 6, 2014, 2014, 5135, 2, 6, 3363, 2, 963, 5135, 963, 2, 1604, 1657, 963, 3363, 963, 1479, 2, 1209, 1604, 1479, 2, 1983, 963, 2, 2, 5135, 1604, 1206, 758, 1964, 963, 2, 1331, 1604, 830, 2, 830, 1604, 2, 1148, 2023, 963, 1479, 13, 590, 2023, 2, 1992, 2023, 6, 830, 963, 1964, 963, 1479, 2, 5135, 1604, 1206, 2, 2023, 6, 1964, 963, 2, 503, 963, 1148, 6, 1206, 590, 963, 2, 1992, 2023, 1604, 2, 2295, 3363, 1604, 1992, 590, 830, 1604, 1983, 1604, 1479, 1479, 1604, 1992, 2, 1983, 6, 5135, 2, 3363, 963, 1964, 963, 1479, 2, 1148, 1604, 1983, 963, 2]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`sequences` must be a list of iterables. Found non-iterable: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6fceb25f2235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m#print(encode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<PAD>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print(encode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpredict1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             raise ValueError('`sequences` must be a list of iterables. '\n\u001b[0;32m---> 64\u001b[0;31m                              'Found non-iterable: ' + str(x))\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `sequences` must be a list of iterables. Found non-iterable: 1"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"review1\",encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        nline = line.replace(\",\",\"\").replace(\".\",\"\").replace(\")\",\"\").replace(\"(\",\"\").replace(\"/\",\"\")\n",
    "        encode = review_encode(nline)\n",
    "        #print(encode)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences(encode, value=word_index[\"<PAD>\"], padding='post',maxlen=250)\n",
    "        #print(encode)\n",
    "        predict1= model1.predict(encode)\n",
    "        predict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
